# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import json
import logging
import tempfile
import time
import threading
import functools
import concurrent.futures
import psutil
import os
from datetime import datetime, timedelta
from dateutil import parser
from typing import Dict, Tuple, List, Optional, Union, Any
from collections import Counter, OrderedDict

# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import boto3
import feedparser
import pytz
import PyRSS2Gen
import requests
from botocore.client import Config
from bs4 import BeautifulSoup
from feedgenerator import DefaultFeed, Enclosure
from ratelimit import limits, sleep_and_retry
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
# –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ (GitHub Actions) –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É–∂–µ –¥–æ—Å—Ç—É–ø–Ω—ã
# –í –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ .env —Ñ–∞–π–ª–∞
try:
    from dotenv import load_dotenv
    dotenv_path = os.path.join(os.path.dirname(__file__), '.env')
    if os.path.exists(dotenv_path):
        load_dotenv(dotenv_path)
        print("Environment variables loaded from .env file (local development)")
    else:
        print("No .env file found - assuming production environment (GitHub Actions)")
except ImportError:
    # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ python-dotenv –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
    print("python-dotenv not available - using system environment variables")

from shared import load_config, send_telegram_message

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
def setup_logging(log_file="../output.log", log_level=logging.INFO):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
    )
    
    # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ñ–∞–π–ª–∞
    file_handler = logging.FileHandler(log_file)
    file_handler.setFormatter(formatter)
    
    # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–Ω—Å–æ–ª–∏
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–æ—Ä–Ω–µ–≤–æ–π –ª–æ–≥–≥–µ—Ä
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    
    # –£–¥–∞–ª–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
        
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    # –í–µ—Ä–Ω—É—Ç—å –ª–æ–≥–≥–µ—Ä –¥–ª—è –º–æ–¥—É–ª—è
    logger = logging.getLogger(__name__)
    return logger

LOGGER = setup_logging()

# –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä–æ–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤—ã–ø–æ–ª–Ω—è—Ç—å –≤ —Å–µ–∫—É–Ω–¥—É.
try:
    rps_value = load_config("RPS")
    CALLS = int(rps_value)
    print(f"RPS loaded successfully: {CALLS}")
except KeyError as e:
    print(f"Warning: RPS environment variable not found. Using default value 1.")
    print("Available environment variables starting with 'R':", [k for k in os.environ.keys() if k.startswith('R')])
    CALLS = 1  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
except ValueError as e:
    print(f"Warning: RPS value '{rps_value}' is not a valid integer. Using default value 1. Error: {e}")
    CALLS = 1  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

PERIOD = 3

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LRU –∫—ç—à
class LRUCache:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LRU –∫—ç—à —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π TTL."""
    
    def __init__(self, max_size=1000, ttl=86400):
        self.max_size = max_size
        self.ttl = ttl
        self.cache = OrderedDict()
        self.lock = threading.RLock()
    
    def get(self, key):
        """–ü–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞."""
        with self.lock:
            if key not in self.cache:
                return None
            
            value, timestamp = self.cache[key]
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ TTL
            if time.time() - timestamp > self.ttl:
                del self.cache[key]
                return None
            
            # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –≤ –∫–æ–Ω–µ—Ü (LRU)
            self.cache.move_to_end(key)
            return value
    
    def set(self, key, value):
        """–î–æ–±–∞–≤–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫—ç—à."""
        with self.lock:
            if key in self.cache:
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
                self.cache[key] = (value, time.time())
                self.cache.move_to_end(key)
            else:
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
                if len(self.cache) >= self.max_size:
                    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–∞–º–æ–≥–æ —Å—Ç–∞—Ä–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                    self.cache.popitem(last=False)
                
                self.cache[key] = (value, time.time())
    
    def clear(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à."""
        with self.lock:
            self.cache.clear()
    
    def size(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞."""
        with self.lock:
            return len(self.cache)

# –£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ API
class ApiCache(LRUCache):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫—ç—à –¥–ª—è API —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
    
    def __init__(self, max_size=1000, ttl=86400):
        super().__init__(max_size, ttl)
        self.hit_count = 0
        self.miss_count = 0
    
    def get(self, key):
        """–ü–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞ —Å —É—á–µ—Ç–æ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏."""
        result = super().get(key)
        if result is not None:
            self.hit_count += 1
        else:
            self.miss_count += 1
        return result
    
    def get_hit_rate(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø–æ–ø–∞–¥–∞–Ω–∏–π –≤ –∫—ç—à."""
        total = self.hit_count + self.miss_count
        return self.hit_count / total if total > 0 else 0

# –ú–µ–Ω–µ–¥–∂–µ—Ä HTTP —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
class ConnectionManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä HTTP —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å –ø—É–ª–∏–Ω–≥–æ–º –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
    
    def __init__(self, max_retries=3, backoff_factor=0.3, timeout=30):
        self.session = requests.Session()
        self.timeout = timeout
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ —Å –ø—É–ª–∏–Ω–≥–æ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20
        )
        
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    def get(self, url, **kwargs):
        """GET –∑–∞–ø—Ä–æ—Å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
        kwargs.setdefault('timeout', self.timeout)
        return self.session.get(url, **kwargs)
    
    def post(self, url, **kwargs):
        """POST –∑–∞–ø—Ä–æ—Å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
        kwargs.setdefault('timeout', self.timeout)
        return self.session.post(url, **kwargs)
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å —Å–µ—Å—Å–∏—é."""
        self.session.close()

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API
class ApiMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API."""
    
    def __init__(self, quota_limit=10000):
        self.quota_limit = quota_limit
        self.calls_today = 0
        self.errors = Counter()
        self.response_times = []
        self.last_reset = datetime.now().date()
        self.lock = threading.Lock()
        
    def record_call(self, response_time=None, error=None):
        """–ó–∞–ø–∏—Å–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–∑–æ–≤–µ API."""
        with self.lock:
            # –°–±—Ä–æ—Å–∏—Ç—å —Å—á–µ—Ç—á–∏–∫–∏ –ø—Ä–∏ —Å–º–µ–Ω–µ –¥–Ω—è
            today = datetime.now().date()
            if today != self.last_reset:
                self.calls_today = 0
                self.last_reset = today
                
            self.calls_today += 1
            
            if response_time is not None:
                self.response_times.append(response_time)
                # –•—Ä–∞–Ω–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 1000 –∑–Ω–∞—á–µ–Ω–∏–π
                if len(self.response_times) > 1000:
                    self.response_times.pop(0)
                    
            if error:
                self.errors[error] += 1
    
    def get_stats(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API."""
        with self.lock:
            avg_response_time = sum(self.response_times) / len(self.response_times) if self.response_times else 0
            
            return {
                "calls_today": self.calls_today,
                "quota_remaining": max(0, self.quota_limit - self.calls_today),
                "quota_used_percent": (self.calls_today / self.quota_limit) * 100 if self.quota_limit else 0,
                "avg_response_time": avg_response_time,
                "errors": dict(self.errors),
            }
    
    def log_stats(self, logger=None):
        """–ó–∞–ø–∏—Å–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –ª–æ–≥."""
        if logger is None:
            logger = LOGGER
            
        stats = self.get_stats()
        logger.info(f"API Stats: {json.dumps(stats, indent=2)}")

# –ö–ª–∞—Å—Å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
class PerformanceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã."""
    
    def __init__(self):
        self.start_time = time.time()
        self.start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        self.checkpoints = []
    
    def add_checkpoint(self, name: str):
        """–î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Ç–æ—á–∫—É."""
        current_time = time.time()
        current_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        self.checkpoints.append({
            'name': name,
            'time': current_time - self.start_time,
            'memory': current_memory,
            'memory_delta': current_memory - self.start_memory
        })
        
        LOGGER.info(f"Checkpoint '{name}': {current_time - self.start_time:.2f}s, "
                   f"Memory: {current_memory:.1f}MB (Œî{current_memory - self.start_memory:+.1f}MB)")
    
    def get_report(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        total_time = time.time() - self.start_time
        current_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        return {
            'total_time': total_time,
            'start_memory': self.start_memory,
            'end_memory': current_memory,
            'memory_peak': max(cp['memory'] for cp in self.checkpoints) if self.checkpoints else current_memory,
            'checkpoints': self.checkpoints,
            'system_info': {
                'cpu_percent': psutil.cpu_percent(),
                'memory_percent': psutil.virtual_memory().percent,
                'pid': os.getpid()
            }
        }

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
api_cache = ApiCache()
api_monitor = ApiMonitor()
connection_manager = ConnectionManager()
performance_monitor = PerformanceMonitor()


def get_previous_feed_and_links(bucket_name: str, s3, object_name) -> Tuple[feedparser.FeedParserDict, List[str]]:
    # –ó–∞–≥—Ä—É–∑–∏—Ç–µ XML –∏–∑ S3
    obj = s3.get_object(Bucket=bucket_name, Key=object_name)
    previous_rss_content = obj['Body'].read().decode('utf-8')
    parsed_rss = feedparser.parse(previous_rss_content)

    # –í–µ—Ä–Ω–∏—Ç–µ —Ä–∞–∑–æ–±—Ä–∞–Ω–Ω—ã–π RSS –∏ —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫
    return parsed_rss, [entry.link for entry in parsed_rss.entries]


def is_entry_recent(entry: feedparser.FeedParserDict, days_ago: datetime) -> tuple:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∑–∞–ø–∏—Å—å –±—ã–ª–∞ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ –Ω–µ –ø–æ–∑–¥–Ω–µ–µ, —á–µ–º –¥–≤–∞ –¥–Ω—è –Ω–∞–∑–∞–¥."""
    pub_date_str = entry.get("published", None)
    if pub_date_str:
        try:
            pub_date_dt = parser.parse(pub_date_str)
            later = pub_date_dt >= days_ago
        except ValueError:
            pub_date_dt = datetime.now(pytz.utc)
            print(pub_date_str)
    else:
        return False, datetime.now(pytz.utc)

    return later, pub_date_dt


def upload_file_to_yandex(file_name: str, bucket: str, s3, object_name) -> None:
    if object_name is None:
        object_name = file_name

    s3.upload_file(file_name, bucket, object_name)
    LOGGER.info(f"File {object_name} uploaded to {bucket}/{object_name}.")


def extract_image_url(summary: Optional[str], logo: str) -> str:
    if summary is None:
        LOGGER.error("Error: No content downloaded")
        return logo
    soup = BeautifulSoup(summary, features="html.parser")
    image_tag = soup.find('img')
    if image_tag and 'src' in image_tag.attrs:
        image_url = image_tag['src']
    else:
        image_url = logo
    return image_url


def adaptive_rate_limit(calls, period, backoff_factor=2, max_backoff=60):
    """–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤."""
    def decorator(func):
        last_calls = []
        lock = threading.Lock()
        
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            with lock:
                now = time.time()
                # –£–¥–∞–ª–∏—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å—ã –æ –≤—ã–∑–æ–≤–∞—Ö
                last_calls[:] = [t for t in last_calls if now - t < period]
                
                if len(last_calls) >= calls:
                    # –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –≤—ã–∑–æ–≤–æ–≤
                    sleep_time = period - (now - last_calls[0])
                    if sleep_time > 0:
                        # –ü—Ä–∏–º–µ–Ω–∏—Ç—å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ —á–∞—Å—Ç—ã—Ö –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è—Ö
                        if len(last_calls) > calls * 1.5:
                            sleep_time = min(sleep_time * backoff_factor, max_backoff)
                        LOGGER.info(f"Rate limit reached. Sleeping for {sleep_time:.2f} seconds")
                        time.sleep(sleep_time)
                
                # –î–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—É—â–∏–π –≤—ã–∑–æ–≤
                last_calls.append(time.time())
                
                return func(*args, **kwargs)
        return wrapper
    return decorator

@adaptive_rate_limit(calls=CALLS, period=PERIOD)
def ya300(link, endpoint, token, max_retries=3):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ API.
    
    Args:
        link (str): URL —Å—Ç–∞—Ç—å–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        endpoint (str): Endpoint API
        token (str): –¢–æ–∫–µ–Ω –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        max_retries (int, optional): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 3.
        
    Returns:
        str or None: URL —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if not link or not endpoint or not token:
        LOGGER.error("Missing required parameters for API call")
        api_monitor.record_call(error="missing_params")
        return None
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
    cached_result = api_cache.get(link)
    if cached_result:
        LOGGER.info(f"Using cached result for link: {link}")
        return cached_result
    
    start_time = time.time()
    
    for retry in range(max_retries):
        try:
            LOGGER.info(f"Sending request to endpoint with link: {link} (attempt {retry+1}/{max_retries})")
            response = connection_manager.post(
                endpoint,
                json={'article_url': link},
                headers={'Authorization': f"OAuth {token}"}
            )
            
            response_time = time.time() - start_time
            LOGGER.info(f"Response status code: {response.status_code} (time: {response_time:.2f}s)")
            
            if response.status_code == 200:
                try:
                    response_data = response.json()
                    url = response_data.get("sharing_url")
                    
                    # –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —É—Å–ø–µ—à–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    if url:
                        api_cache.set(link, url)
                    
                    api_monitor.record_call(response_time=response_time)
                    return url
                except json.JSONDecodeError as e:
                    LOGGER.error(f"JSONDecodeError in ya300: {e}")
                    api_monitor.record_call(response_time=response_time, error="json_error")
                    if retry < max_retries - 1:
                        continue
                    return None
            elif response.status_code == 429:  # Too Many Requests
                wait_time = min(2 ** retry, 60)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞, –º–∞–∫—Å–∏–º—É–º 60 —Å–µ–∫—É–Ω–¥
                LOGGER.warning(f"Rate limit exceeded. Waiting {wait_time} seconds before retry.")
                api_monitor.record_call(error="rate_limit")
                time.sleep(wait_time)
                continue
            else:
                error_msg = f"API error: {response.status_code}"
                LOGGER.warning(error_msg)
                api_monitor.record_call(response_time=response_time, error=f"http_{response.status_code}")
                
                # –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –æ—à–∏–±–æ–∫ –Ω–µ—Ç —Å–º—ã—Å–ª–∞ –ø–æ–≤—Ç–æ—Ä—è—Ç—å –∑–∞–ø—Ä–æ—Å
                if response.status_code in [400, 401, 403, 404]:
                    return None
                
                if retry < max_retries - 1:
                    wait_time = min(2 ** retry, 30)
                    LOGGER.info(f"Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                    continue
                
                return None
                
        except requests.exceptions.ConnectionError as e:
            LOGGER.error(f"ConnectionError in ya300: {e}")
            api_monitor.record_call(error="connection_error")
            if retry < max_retries - 1:
                wait_time = min(2 ** retry, 30)
                LOGGER.info(f"Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                return None
        except Exception as e:
            LOGGER.error(f"Unexpected error occurred in ya300: {e}", exc_info=True)
            api_monitor.record_call(error="unknown_error")
            return None
    
    return None


def parse_summary_page(sum_link: str) -> Optional[str]:
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç HTML-–∫–æ–Ω—Ç–µ–Ω—Ç.
    
    Args:
        sum_link (str): URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π
        
    Returns:
        Optional[str]: HTML-–∫–æ–Ω—Ç–µ–Ω—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    cached_result = api_cache.get(f"parse_{sum_link}")
    if cached_result:
        LOGGER.info(f"Using cached parsing result for: {sum_link}")
        return cached_result
    
    try:
        response = connection_manager.get(sum_link)
        if response.status_code != 200:
            LOGGER.warning(f"Failed to fetch summary page: {response.status_code}")
            return None
            
        webpage = response.content
        soup = BeautifulSoup(webpage, 'html.parser')
        
        # –ù–∞—Ö–æ–¥–∏–º div —Å –∫–ª–∞—Å—Å–æ–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º "summary-text"
        div_element = soup.find('div', class_=lambda value: value and 'summary-text' in value)
        if not div_element:
            LOGGER.warning("Summary div not found in the page")
            return None
            
        result_html = ''
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        h1_element = div_element.find('h1', class_='title')
        if h1_element:
            h1_text = h1_element.get_text(strip=True)
            result_html += f'<h1>{h1_text}</h1>\n'
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∑–∏—Å—ã
        li_elements = div_element.select('ul.theses li[class*=thesis]')
        if li_elements:
            for li in li_elements:
                thesis_element = li.find('p', class_='thesis-text')
                if thesis_element:
                    li_text = thesis_element.get_text(strip=True)
                    result_html += f'<p>{li_text}</p>\n'
        else:
            LOGGER.warning("No thesis elements found in the summary")
        
        # –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞
        if result_html:
            api_cache.set(f"parse_{sum_link}", result_html)
            
        return result_html if result_html else None
        
    except Exception as e:
        LOGGER.error(f"Error parsing summary page: {e}", exc_info=True)
        return None

def process_special_source(entry: feedparser.FeedParserDict, logo: str) -> Tuple[str, str]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø–∏—Å–∏ –∏–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (Telegram, Radio-T).
    
    Args:
        entry (feedparser.FeedParserDict): –ó–∞–ø–∏—Å—å RSS
        logo (str): URL –ª–æ–≥–æ—Ç–∏–ø–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
    Returns:
        Tuple[str, str]: (summary, image_url)
    """
    summary = f"{entry['summary']} <a href='{entry['link']}'>–ß–∏—Ç–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª</a>"
    im_url = extract_image_url(entry['summary'], logo)
    return summary, im_url

def process_entry(entry: feedparser.FeedParserDict, days_ago: datetime, previous_links: List[str], logo: str, endpoint,
                  token) -> Optional[Dict[str, Union[str, Enclosure]]]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø–∏—Å—å RSS –∏ —Å–æ–∑–¥–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∏–¥–∞.
    
    Args:
        entry (feedparser.FeedParserDict): –ó–∞–ø–∏—Å—å RSS
        days_ago (datetime): –î–∞—Ç–∞, —Ä–∞–Ω—å—à–µ –∫–æ—Ç–æ—Ä–æ–π –∑–∞–ø–∏—Å–∏ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è
        previous_links (List[str]): –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ñ–∏–¥–∞
        logo (str): URL –ª–æ–≥–æ—Ç–∏–ø–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        endpoint (str): Endpoint API —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        token (str): –¢–æ–∫–µ–Ω –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ API
        
    Returns:
        Optional[Dict[str, Union[str, Enclosure]]]: –î–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∏–¥ –∏–ª–∏ None
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        pub_date = parser.parse(entry.published).astimezone(pytz.utc)
        if pub_date < days_ago:
            return None
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
        if entry['link'] in previous_links:
            return None
            
        im_url = logo
        summary = ""
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        if entry['link'].startswith("https://t.me") or entry['link'].startswith("https://radio-t.com"):
            summary, im_url = process_special_source(entry, logo)
        else:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ API
            sum_link = ya300(entry['link'], endpoint, token)
            LOGGER.info(f"Summarization link: {sum_link}")
            
            if sum_link is None:
                # –ï—Å–ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                summary = f"{entry['summary']} <a href='{entry['link']}'>–ß–∏—Ç–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª</a>"
            else:
                # –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
                result_html = parse_summary_page(sum_link)
                
                if result_html:
                    summary = f"{result_html} <a href='{entry['link']}'>–ß–∏—Ç–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª</a>"
                else:
                    summary = f"{entry['summary']} <a href='{entry['link']}'>–ß–∏—Ç–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª</a>"
                    
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            im_url = extract_image_url(entry['summary'], logo)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        return {
            'title': entry['title'],
            'link': entry['link'],
            'description': summary,
            'pubdate': pub_date
        }
        
    except Exception as e:
        LOGGER.error(f"Error processing entry {entry.get('link', 'unknown')}: {e}", exc_info=True)
        return None

def process_entries_batch(entries: List[feedparser.FeedParserDict], days_ago: datetime, 
                         previous_links: List[str], logo: str, endpoint: str, token: str,
                         max_workers: int = 3) -> List[Dict[str, Union[str, Enclosure]]]:
    """
    –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–µ–π RSS —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º.
    
    Args:
        entries (List[feedparser.FeedParserDict]): –°–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π RSS
        days_ago (datetime): –î–∞—Ç–∞, —Ä–∞–Ω—å—à–µ –∫–æ—Ç–æ—Ä–æ–π –∑–∞–ø–∏—Å–∏ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è
        previous_links (List[str]): –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ñ–∏–¥–∞
        logo (str): URL –ª–æ–≥–æ—Ç–∏–ø–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        endpoint (str): Endpoint API —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        token (str): –¢–æ–∫–µ–Ω –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ API
        max_workers (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        
    Returns:
        List[Dict[str, Union[str, Enclosure]]]: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
    """
    processed_entries = []
    
    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–ø–∏—Å–µ–π
    valid_entries = []
    for entry in entries:
        try:
            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç—ã –±–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥–∞
            if hasattr(entry, 'published'):
                pub_date = parser.parse(entry.published).astimezone(pytz.utc)
                if pub_date >= days_ago and entry.get('link') not in previous_links:
                    valid_entries.append(entry)
        except Exception as e:
            LOGGER.warning(f"Error in preliminary filtering: {e}")
            continue
    
    LOGGER.info(f"Processing {len(valid_entries)} valid entries out of {len(entries)} total")
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_entry = {
            executor.submit(process_entry, entry, days_ago, previous_links, logo, endpoint, token): entry 
            for entry in valid_entries
        }
        
        for future in concurrent.futures.as_completed(future_to_entry):
            entry = future_to_entry[future]
            try:
                result = future.result()
                if result:
                    processed_entries.append(result)
            except Exception as exc:
                LOGGER.error(f"Entry {entry.get('link', 'unknown')} generated an exception: {exc}", exc_info=True)
    
    return processed_entries


def process_single_feed(feed_url: str) -> List[PyRSS2Gen.RSSItem]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω RSS —Ñ–∏–¥.
    
    Args:
        feed_url (str): URL RSS —Ñ–∏–¥–∞
        
    Returns:
        List[PyRSS2Gen.RSSItem]: –°–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ RSS –∏–∑ —Ñ–∏–¥–∞
    """
    items = []
    
    if not feed_url.strip():
        return items
    
    try:
        LOGGER.info(f"Processing feed: {feed_url}")
        feed_response = connection_manager.get(feed_url)
        
        if feed_response.status_code != 200:
            LOGGER.warning(f"Failed to fetch feed {feed_url}: {feed_response.status_code}")
            return items
            
        feed = feedparser.parse(feed_response.content)
        
        if feed.bozo:
            LOGGER.warning(f"Feed {feed_url} has errors: {feed.bozo_exception}")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–µ–π
        for entry in feed.entries:
            try:
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                pub_date = get_entry_date(entry)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞ RSS
                item = PyRSS2Gen.RSSItem(
                    title=entry.title if hasattr(entry, "title") else "No title",
                    link=entry.link if hasattr(entry, "link") else "No link",
                    description=entry.description if hasattr(entry, "description") else "No description",
                    guid=PyRSS2Gen.Guid(entry.link if hasattr(entry, "link") else "No link"),
                    pubDate=pub_date
                )
                items.append(item)
            except Exception as entry_error:
                LOGGER.error(f"Error processing entry in feed {feed_url}: {entry_error}", exc_info=True)
                continue
                
        LOGGER.info(f"Processed {len(feed.entries)} entries from {feed_url}")
        
    except requests.exceptions.RequestException as e:
        LOGGER.error(f"Request error for feed {feed_url}: {e}")
    except Exception as e:
        LOGGER.error(f"Error processing feed {feed_url}: {e}", exc_info=True)
    
    return items

def merge_rss_feeds(url: str, max_workers: int = 5) -> List[PyRSS2Gen.RSSItem]:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ RSS-—Ñ–∏–¥–æ–≤ –≤ –æ–¥–∏–Ω —Å–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π.
    
    Args:
        url (str): URL —Ñ–∞–π–ª–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º URL RSS-—Ñ–∏–¥–æ–≤
        max_workers (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        
    Returns:
        List[PyRSS2Gen.RSSItem]: –°–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ RSS
    """
    items = []
    
    try:
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ URL
        response = connection_manager.get(url)
        if response.status_code != 200:
            LOGGER.error(f"Failed to fetch RSS list: {response.status_code}")
            return items
            
        urls = [url.strip() for url in response.text.splitlines() if url.strip()]
        LOGGER.info(f"Found {len(urls)} RSS feeds to process")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∏–¥–æ–≤
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(process_single_feed, feed_url): feed_url for feed_url in urls}
            
            for future in concurrent.futures.as_completed(future_to_url):
                feed_url = future_to_url[future]
                try:
                    feed_items = future.result()
                    items.extend(feed_items)
                    LOGGER.info(f"Successfully processed {len(feed_items)} items from {feed_url}")
                except Exception as exc:
                    LOGGER.error(f"Feed {feed_url} generated an exception: {exc}", exc_info=True)
                
    except requests.exceptions.RequestException as e:
        LOGGER.error(f"Request error for RSS list URL {url}: {e}")
    except Exception as e:
        LOGGER.error(f"Error merging RSS feeds: {e}", exc_info=True)
        
    LOGGER.info(f"Total items collected: {len(items)}")
    return items

def get_entry_date(entry: feedparser.FeedParserDict) -> datetime:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–∑ –∑–∞–ø–∏—Å–∏ RSS.
    
    Args:
        entry (feedparser.FeedParserDict): –ó–∞–ø–∏—Å—å RSS
        
    Returns:
        datetime: –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–ª–∏ —Ç–µ–∫—É—â–∞—è –¥–∞—Ç–∞, –µ—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
    """
    try:
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∞—Ç—ã
            return datetime(*entry.published_parsed[:6]).replace(tzinfo=pytz.utc)
        elif hasattr(entry, "published"):
            # –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ –¥–∞—Ç—ã
            return parser.parse(entry.published).replace(tzinfo=pytz.utc)
        elif hasattr(entry, "pub_date"):
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –ø–æ–ª–µ –¥–∞—Ç—ã
            return parser.parse(entry.pub_date).replace(tzinfo=pytz.utc)
        elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            return datetime(*entry.updated_parsed[:6]).replace(tzinfo=pytz.utc)
        elif hasattr(entry, "updated"):
            # –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ –¥–∞—Ç—ã –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            return parser.parse(entry.updated).replace(tzinfo=pytz.utc)
        else:
            # –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é
            return datetime.now(pytz.utc)
    except (ValueError, TypeError) as e:
        LOGGER.warning(f"Error parsing date for entry {entry.get('title', 'unknown')}: {e}")
        return datetime.now(pytz.utc)


def create_new_rss(items: List[PyRSS2Gen.RSSItem], config: Optional[Dict[str, Any]] = None) -> PyRSS2Gen.RSS2:
    """
    –°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π RSS-—Ñ–∏–¥ –∏–∑ —Å–ø–∏—Å–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤.
    
    Args:
        items (List[PyRSS2Gen.RSSItem]): –°–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ RSS
        config (Optional[Dict[str, Any]]): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ñ–∏–¥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        PyRSS2Gen.RSS2: –û–±—ä–µ–∫—Ç RSS-—Ñ–∏–¥–∞
    """
    if config is None:
        config = {}
        
    # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    title = config.get("feed_title", "–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π RSS —Ñ–∏–¥")
    link = config.get("feed_link", "http://example.com/new-feed.xml")
    description = config.get("feed_description", "–ù–æ–≤—ã–π RSS —Ñ–∏–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–¥–∞
    rss = PyRSS2Gen.RSS2(
        title=title,
        link=link,
        description=description,
        lastBuildDate=datetime.now(pytz.utc),
        items=items,
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        generator=config.get("generator", "RSS Summarizer"),
        docs=config.get("docs", "https://cyber.harvard.edu/rss/rss.html"),
        language=config.get("language", "ru")
    )
    
    return rss


def load_app_config() -> Dict[str, Any]:
    """
    –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.
    
    Returns:
        Dict[str, Any]: –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    """
    try:
        config = {
            # API –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            "endpoint_300": load_config("endpoint_300"),
            "token_300": load_config("token_300"),
            "api_calls_limit": int(load_config("RPS")),
            "api_period": 3,
            
            # S3 –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            "bucket_name": load_config("BUCKET_NAME"),
            "endpoint_url": load_config("ENDPOINT_URL"),
            "access_key": load_config("ACCESS_KEY"),
            "secret_key": load_config("SECRET_KEY"),
            "rss_file_name": load_config("rss_300_file_name"),
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ RSS
            "logo_url": load_config("logo_url"),
            "rss_links": load_config("RSS_LINKS"),
            "days_to_keep": 7,
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Telegram
            "telegram_token": load_config("TELEGRAM_BOT_TOKEN"),
            "telegram_chat_id": load_config("TELEGRAM_CHAT_ID"),
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–¥–∞
            "feed_title": "Dzarlax Feed",
            "feed_link": "https://s3.dzarlax.dev/feed.rss",
            "feed_description": "Front Page articles from Dzarlax, summarized with AI"
        }
        
        return config
    except KeyError as e:
        LOGGER.error(f"Missing required configuration key: {e}")
        raise
    except Exception as e:
        LOGGER.error(f"Error loading configuration: {e}", exc_info=True)
        raise

def init_s3_client(config: Dict[str, Any]):
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–µ–Ω—Ç S3.
    
    Args:
        config (Dict[str, Any]): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        
    Returns:
        boto3.client: –ö–ª–∏–µ–Ω—Ç S3
    """
    try:
        s3 = boto3.client('s3',
                      endpoint_url=config["endpoint_url"],
                      aws_access_key_id=config["access_key"],
                      aws_secret_access_key=config["secret_key"],
                      config=Config(signature_version='s3v4'))
        return s3
    except Exception as e:
        LOGGER.error(f"Failed to initialize S3 client: {e}", exc_info=True)
        raise

def process_previous_entries(previous_feed, days_ago, logo):
    """
    –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø–∏—Å–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ñ–∏–¥–∞.
    
    Args:
        previous_feed (feedparser.FeedParserDict): –ü—Ä–µ–¥—ã–¥—É—â–∏–π —Ñ–∏–¥
        days_ago (datetime): –î–∞—Ç–∞, —Ä–∞–Ω—å—à–µ –∫–æ—Ç–æ—Ä–æ–π –∑–∞–ø–∏—Å–∏ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è
        logo (str): URL –ª–æ–≥–æ—Ç–∏–ø–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∏–¥
    """
    entries = []
    
    for entry in previous_feed.entries:
        later, pub_date_dt = is_entry_recent(entry, days_ago)
        if not later:
            continue
            
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if 'enclosures' in entry and len(entry.enclosures) > 0:
            # Check if the first enclosure has an 'href' attribute
            if 'href' in entry.enclosures[0]:
                enclosure_href = entry.enclosures[0]['href']
            else:
                enclosure_href = logo  # or some default image URL
        else:
            enclosure_href = logo  # or some default image URL

        entries.append({
            'title': entry.title,
            'link': entry.link,
            'description': entry.description,
            'enclosure': Enclosure(enclosure_href, '1234', 'image/jpeg'),
            'pubdate': pub_date_dt
        })
        
    return entries

def create_output_feed(config, previous_entries, new_entries):
    """
    –°–æ–∑–¥–∞—Ç—å –≤—ã—Ö–æ–¥–Ω–æ–π RSS-—Ñ–∏–¥.
    
    Args:
        config (Dict[str, Any]): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        previous_entries (List[Dict]): –ó–∞–ø–∏—Å–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ñ–∏–¥–∞
        new_entries (List[Dict]): –ù–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
        
    Returns:
        str: RSS-—Ñ–∏–¥ –≤ —Ñ–æ—Ä–º–∞—Ç–µ XML
    """
    out_feed = DefaultFeed(
        title=config["feed_title"],
        link=config["feed_link"],
        description=config["feed_description"]
    )
    
    # –î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø–∏—Å–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ñ–∏–¥–∞
    for entry in previous_entries:
        out_feed.add_item(**entry)
    
    # –î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
    for entry in new_entries:
        if entry:  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None
            out_feed.add_item(**entry)
    
    return out_feed.writeString('utf-8')

def main_func() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    start_time = time.time()
    LOGGER.info("Starting RSS summarization process")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = load_app_config()
        performance_monitor.add_checkpoint("Config loaded")
        LOGGER.info("Configuration loaded successfully")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è S3 –∫–ª–∏–µ–Ω—Ç–∞
        s3 = init_s3_client(config)
        performance_monitor.add_checkpoint("S3 initialized")
        LOGGER.info("S3 client initialized")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ñ–∏–¥–∞
        previous_feed, previous_links = get_previous_feed_and_links(
            config["bucket_name"], 
            s3, 
            config["rss_file_name"]
        )
        performance_monitor.add_checkpoint("Previous feed loaded")
        LOGGER.info(f"Previous feed loaded with {len(previous_links)} entries")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        logo = config["logo_url"]
        days_ago = datetime.now(pytz.utc) - timedelta(days=config["days_to_keep"])
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
        items = merge_rss_feeds(config["rss_links"])
        performance_monitor.add_checkpoint("RSS feeds merged")
        LOGGER.info(f"Merged RSS feeds, got {len(items)} items")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ñ–∏–¥–∞
        rss_string = create_new_rss(items, config).to_xml('utf-8')
        in_feed = feedparser.parse(rss_string)
        performance_monitor.add_checkpoint("New feed created")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–ø–∏—Å–µ–π
        previous_entries = process_previous_entries(previous_feed, days_ago, logo)
        LOGGER.info(f"Processed {len(previous_entries)} entries from previous feed")
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        sorted_entries = sorted(
            in_feed.entries,
            key=lambda entry: parser.parse(entry.published),
            reverse=True
        )
        performance_monitor.add_checkpoint("Entries sorted")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π —Å –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        new_entries = process_entries_batch(
            sorted_entries,
            days_ago,
            previous_links,
            logo,
            config["endpoint_300"],
            config["token_300"],
            max_workers=3  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤
        )
        performance_monitor.add_checkpoint("New entries processed")
        
        LOGGER.info(f"Processed {len(new_entries)} new entries")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∏–¥–∞
        rss = create_output_feed(config, previous_entries, new_entries)
        performance_monitor.add_checkpoint("Output feed created")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∏–¥–∞ –≤ S3
        with tempfile.NamedTemporaryFile(suffix=".xml") as temp:
            temp.write(rss.encode('utf-8'))
            upload_file_to_yandex(temp.name, config["bucket_name"], s3, config["rss_file_name"])
        performance_monitor.add_checkpoint("Feed uploaded to S3")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        api_monitor.log_stats()
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞
        cache_hit_rate = api_cache.get_hit_rate()
        cache_size = api_cache.size()
        LOGGER.info(f"Cache statistics: Hit rate: {cache_hit_rate:.2%}, Size: {cache_size}")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        performance_report = performance_monitor.get_report()
        LOGGER.info(f"Performance report: {json.dumps(performance_report, indent=2, default=str)}")
        
        elapsed_time = time.time() - start_time
        LOGGER.info(f"RSS summarization completed successfully in {elapsed_time:.2f} seconds")
        
        # –û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ–± —É—Å–ø–µ—à–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        success_message = (f"‚úÖ RSS –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞ {elapsed_time:.2f}—Å\n"
                          f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(new_entries)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π\n"
                          f"üíæ –ö—ç—à: {cache_hit_rate:.1%} –ø–æ–ø–∞–¥–∞–Ω–∏–π, —Ä–∞–∑–º–µ—Ä: {cache_size}\n"
                          f"üîß –ü–∞–º—è—Ç—å: {performance_report['memory_peak']:.1f}MB –ø–∏–∫")
        send_telegram_message(
            success_message, 
            config.get("telegram_token", ""), 
            config.get("telegram_chat_id", "")
        )
        
    except KeyError as e:
        error_message = f"Missing configuration key: {e}"
        LOGGER.error(error_message)
        send_telegram_message(
            error_message, 
            config.get("telegram_token", ""), 
            config.get("telegram_chat_id", "")
        )
    except json.JSONDecodeError as e:
        error_message = f"JSON decode error: {e}"
        LOGGER.error(error_message)
        send_telegram_message(
            error_message, 
            config.get("telegram_token", ""), 
            config.get("telegram_chat_id", "")
        )
    except Exception as e:
        error_message = f"Application crashed with error: {e}"
        LOGGER.error(error_message, exc_info=True)
        
        # –ü–æ–ø—ã—Ç–∫–∞ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –≤ Telegram
        try:
            send_telegram_message(
                error_message, 
                config.get("telegram_token", ""), 
                config.get("telegram_chat_id", "")
            )
        except Exception as telegram_error:
            LOGGER.error(f"Failed to send Telegram notification: {telegram_error}")
    
    finally:
        # –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
        try:
            connection_manager.close()
            LOGGER.info("Connection manager closed successfully")
        except Exception as cleanup_error:
            LOGGER.error(f"Error during cleanup: {cleanup_error}")


if __name__ == "__main__":
    main_func()
