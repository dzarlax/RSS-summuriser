#!/usr/bin/env python
# coding: utf-8
import datetime
import re
import sys
from urllib.parse import urlparse
from xml.etree import ElementTree as ET

import pandas as pd
import requests
from bs4 import BeautifulSoup
from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import connected_components
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from telegraph import Telegraph

from shared import load_config, send_telegram_message, convert_markdown_to_html

API_URL = load_config("CONSTRUCTOR_KM_API")
API_KEY = load_config("CONSTRUCTOR_KM_API_KEY")
model = load_config("MODEL")

if len(sys.argv) > 1:
    # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –∞—Ä–≥—É–º–µ–Ω—Ç–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    infra = sys.argv[1]
    # –¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –≤ –≤–∞—à–µ–º —Å–∫—Ä–∏–ø—Ç–µ
    print(f"–ü–µ—Ä–µ–¥–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {infra}")
else:
    infra = 'prod'
    print("–ê—Ä–≥—É–º–µ–Ω—Ç –Ω–µ –±—ã–ª –ø–µ—Ä–µ–¥–∞–Ω.")


def fetch_and_parse_rss_feed(url: str) -> pd.DataFrame:
    response = requests.get(url)
    root = ET.fromstring(response.content)
    data = [{'headline': item.find('title').text,
             'link': item.find('link').text,
             'pubDate': datetime.datetime.strptime(item.find('pubDate').text, '%a, %d %b %Y %H:%M:%S %z').date(),
             'description': item.find('description').text} for item in root.findall('.//item')]
    return pd.DataFrame(data)


# Simple inference example

def clean_html(html_text, max_length=1000):
    soup = BeautifulSoup(html_text, "html.parser")
    clean_text = soup.get_text()
    if len(clean_text) > max_length:
        clean_text = clean_text[:max_length]
    return clean_text

def generate_summary_batch(input_texts: list, batch_size: int = 4, ) -> list:
    summaries = []
    for i in range(0, len(input_texts), batch_size):
        batch_texts = input_texts[i:i + batch_size]
        batch_prompts = ["Choose one of the provided categories and answer with one word (Business, Tech, Science, Nature, Serbia, Marketing, Other) for the article:" + clean_html(text) for text in batch_texts]
        for prompt in batch_prompts:
            summary = process_with_gpt(prompt)
            summaries.append(summary)

    return summaries


def process_with_gpt(prompt):
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json",
        "X-KM-AccessKey": API_KEY
    }
    data = {
        "messages": [
            {
                "role": "user",
                "content": prompt,
                "name": "summary_request"
            }
        ],
        "model": model
    }

    response = requests.post(API_URL, json=data, headers=headers)

    if response.status_code == 200:
        response_json = response.json()
        output_text = response_json["choices"][0]["message"]["content"]

        # If generating overview, return full text
        if "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π" in prompt:
            return output_text
        # For category classification, return first word
        else:
            first_word = output_text.split()[0]
            cleaned_first_word = re.sub(r'[^a-zA-Z]', '', first_word)
            return cleaned_first_word
    else:
        print(f"–û—à–∏–±–∫–∞ API: {response.status_code} - {response.text}")
        return "Error"




def deduplication(data):
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ TF-IDF –∏ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(data['headline'])
    cosine_sim_matrix = cosine_similarity(tfidf_matrix)

    # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≥—Ä—É–ø–ø –Ω–æ–≤–æ—Å—Ç–µ–π
    threshold = 0.5
    graph = csr_matrix(cosine_sim_matrix > threshold)
    n_components, labels = connected_components(csgraph=graph, directed=False, return_labels=True)
    data['group_id'] = labels

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ group_id –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ –≤ —Å–ø–∏—Å–∫–∏
    links_aggregated = data.groupby('group_id')['link'].apply(list).reset_index()

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å–∞–º—ã–º –¥–ª–∏–Ω–Ω—ã–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ
    longest_headlines = data.loc[data.groupby('group_id')['headline'].apply(lambda x: x.str.len().idxmax())]

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —á—Ç–æ–±—ã –∫ –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫
    result = pd.merge(longest_headlines, links_aggregated, on='group_id', how='left')

    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏
    result.rename(columns={'link_x': 'link', 'link_y': 'links'}, inplace=True)

    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –Ω–µ –≤–∫–ª—é—á–∞—è —Å—Ç–æ–ª–±–µ—Ü 'links'
    cols_for_deduplication = [col for col in result.columns if col != 'links']
    result = result.drop_duplicates(subset=cols_for_deduplication)
    return result


def escape_html(text):
    """–ó–∞–º–µ–Ω—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ HTML —Å–∏–º–≤–æ–ª—ã –Ω–∞ –∏—Ö —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—ã."""
    return text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')


def format_html_telegram(row):
    # –≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö HTML —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
    headline = escape_html(row['headline'])
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è HTML –∏–∑ —Å–ø–∏—Å–∫–∞ URL
    links_formatted = ['<a href="{0}">{1}</a>'.format('https://dzarlax.dev/rss/articles/article.html?link=' + link, urlparse(link).netloc) for link in row['links']]
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ HTML –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ —Å–ø–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫
    links_html = '\n'.join(links_formatted)
    return f"{headline}\n{links_html}\n"


def html4tg(result):
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è Telegram —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º HTML
    html_output_telegram = ""
    for category, group in result.groupby('category'):
        category_html = category.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
        html_output_telegram += f"\n\n<b>{category_html}</b>\n\n"
        html_output_telegram += '\n'.join(group.apply(format_html_telegram, axis=1))
    return html_output_telegram


def create_telegraph_page_with_library(result, access_token, author_name="Dzarlax", author_url="https://dzarlax.dev"):
    telegraph = Telegraph(access_token=access_token)
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ HTML, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏
    content_html = ""
    for category, group in result.groupby('category'):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º <h3> –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π, —Ç.–∫. <h2> –≤ —Å–ø–∏—Å–∫–µ –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã—Ö
        content_html += f"<hr><h3>{category}</h3>"
        print(category)

        for _, row in group.iterrows():
            article_title = row['headline']
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –≤ <ul>
            links_html = ''.join([f'<a href=https://dzarlax.dev/rss/articles/article.html?link={link}>{urlparse(link).netloc}</a>' for link in row['links']])
            # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–∞—Ç–µ–π –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ <p> –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∫ –Ω–∏–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫
            content_html += f"<ul><p>{article_title}  {links_html}</p></ul>\n"

    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ Telegra.ph
    response = telegraph.create_page(
        title="–ù–æ–≤–æ—Å—Ç–∏ –∑–∞ " + str(datetime.datetime.now().date()),
        html_content=content_html,
        author_name=author_name,
        author_url=author_url
    )
    return response['url']


# def generate_daily_overview(result):
#     # Prepare prompt in Russian
#     prompt = """–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π (500-4000 —Å–∏–º–≤–æ–ª–æ–≤) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:
#
# """
#     # Add categorized headlines to prompt
#     for category, group in result.groupby('category'):
#         # Translate category names to Russian
#         category_ru = {
#             'Business': 'üíº –ë–∏–∑–Ω–µ—Å',
#             'Tech': 'üíª –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏',
#             'Science': 'üî¨ –ù–∞—É–∫–∞',
#             'Nature': 'üåø –ü—Ä–∏—Ä–æ–¥–∞',
#             'Serbia': 'üá∑üá∏ –°–µ—Ä–±–∏—è',
#             'Marketing': 'üìä –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥',
#             'Other': 'üìå –î—Ä—É–≥–æ–µ'
#         }.get(category, category)
#
#         prompt += f"\n{category_ru}:\n"
#         for _, row in group.iterrows():
#             prompt += f"- {row['headline']}\n"
#
#     prompt += "\n–°–æ—Å—Ç–∞–≤—å—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—É—é —Å–≤–æ–¥–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π –¥–Ω—è, —Å–æ—Ö—Ä–∞–Ω—è—è –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç—Å–∫–∏–π —Å—Ç–∏–ª—å."
#
#     overview = process_with_gpt(prompt)
#
#     # Ensure the overview doesn't exceed 4000 characters
#     if len(overview) > 4000:
#         overview = overview[:3997] + "..."
#
#     return overview
def generate_daily_overview(result):
    # Prepare prompt in Russian
    prompt = """–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π (500-4000 —Å–∏–º–≤–æ–ª–æ–≤) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏:

"""
    for category, group in result.groupby('category'):
        prompt += f"\n{category}:\n"
        for _, row in group.iterrows():
            # Get headline
            headline = row['headline']

            # Get description and truncate to 400 characters if needed
            description = row.get('description', '')
            # if description:
            #     if len(description) > 400:
            #         description = description[:397] + "..."

            # Add headline and description to prompt
            prompt += f"- {headline}\n"
            if description:
                prompt += f"  {description}\n"

    prompt += "\n–°–æ—Å—Ç–∞–≤—å—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—É—é —Å–≤–æ–¥–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π –¥–Ω—è, —Å–æ—Ö—Ä–∞–Ω—è—è –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç—Å–∫–∏–π —Å—Ç–∏–ª—å."

    overview = process_with_gpt(prompt)

    # Ensure the overview doesn't exceed 4000 characters
    if len(overview) > 4000:
        overview = overview[:3997] + "..."

    return overview

# def generate_daily_overview(result):
#     # Prepare prompt with categorized news
#     prompt = "Generate a comprehensive overview of today's news (between 500-4000 characters) based on these categorized headlines:\n\n"
#
#     for category, group in result.groupby('category'):
#         prompt += f"\n{category}:\n"
#         for _, row in group.iterrows():
#             prompt += f"- {row['headline']}\n"
#
#     prompt += "\nFormat the overview as a readable summary with emoji for each category. Keep it informative but concise."
#
#     overview = process_with_gpt(prompt)
#
#     # Ensure the overview doesn't exceed 4000 characters
#     if len(overview) > 4000:
#         overview = overview[:3997] + "..."
#
#     return overview


def prepare_and_send_message(result, chat_id, telegram_token, telegraph_access_token, service_chat_id):
    # Generate overview in Russian
    daily_overview = generate_daily_overview(result)
    daily_overview_html = convert_markdown_to_html(daily_overview)

    # Create Telegraph page
    telegraph_url = create_telegraph_page_with_library(result, telegraph_access_token)

    # Format message with overview and link
    current_date = datetime.datetime.now().strftime("%d.%m.%Y")
    message = f"üì∞ –°–≤–æ–¥–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ {current_date}\n\n{daily_overview_html}\n\nüîó –ü–æ–¥—Ä–æ–±–Ω–µ–µ: {telegraph_url}"

    response = send_telegram_message(message, chat_id, telegram_token)
    if response.get('ok'):
        send_telegram_message("–°–æ–æ–±—â–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ", service_chat_id, telegram_token)
    else:
        send_telegram_message("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ", service_chat_id, telegram_token)
    return response

def job():
    if infra == 'prod':
        chat_id = load_config("TELEGRAM_CHAT_ID_NEWS")
    elif infra == 'test':
        chat_id = load_config("TELEGRAM_CHAT_ID")

    service_chat_id = load_config("TELEGRAM_CHAT_ID")
    telegram_token = load_config("TELEGRAM_BOT_TOKEN")
    telegraph_access_token = load_config("TELEGRAPH_ACCESS_TOKEN")

    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ñ–∏–¥–∞
    data = fetch_and_parse_rss_feed(load_config("feed_url"))

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    data['today'] = datetime.datetime.now().date()
    data = data[data['pubDate'] == data['today']].drop(columns=['today', 'pubDate'])
    data['category'] = generate_summary_batch(
        data['description'],
        batch_size=4
    )
    result = deduplication(data)
    response = prepare_and_send_message(result, chat_id, telegram_token, telegraph_access_token, service_chat_id)
    print(response)


job()
